{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specified-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-tradition",
   "metadata": {},
   "source": [
    "### 导入 中 - 英 翻译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "female-impression",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:925: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model_zh_en = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "tokenizer_zh_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model_en_zh = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "tokenizer_en_zh = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-model",
   "metadata": {},
   "source": [
    "### 导入 中 - 德 翻译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offshore-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zh_de = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-zh-de\")\n",
    "tokenizer_zh_de = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-de\")\n",
    "model_de_zh = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-de-ZH\")\n",
    "tokenizer_de_zh = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-ZH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-holder",
   "metadata": {},
   "source": [
    "### 使用GPU加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "apparent-today",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(61916, 512, padding_idx=61915)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(61916, 512, padding_idx=61915)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(61916, 512, padding_idx=61915)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=61916, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_zh_en.to(device)\n",
    "model_en_zh.to(device)\n",
    "model_zh_de.to(device)\n",
    "model_de_zh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "applicable-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translation(texts, tokenizer_zh_fg, model_zh_fg, tokenizer_fg_zh, model_fg_zh, max_length=200):\n",
    "    \"\"\"\n",
    "    批量文本翻译\n",
    "    \"\"\"\n",
    "    t = time.time()\n",
    "    fg_texts = []\n",
    "    zh_texts = []\n",
    "    # 中文转外语\n",
    "    embedded_zh = tokenizer_zh_fg.batch_encode_plus(texts, padding=True)['input_ids']\n",
    "    embedded_zh = torch.tensor(embedded_zh).to(device)\n",
    "    embedded_zh = model_zh_fg.generate(embedded_zh, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    for i in range(embedded_zh.size()[0]):\n",
    "        fg_texts.append(re.sub('<pad>', '', tokenizer_zh_fg.decode(embedded_zh[i])))\n",
    "\n",
    "    # 外语转英文\n",
    "    embedded_fg = tokenizer_fg_zh.batch_encode_plus(fg_texts, padding=True)['input_ids']\n",
    "    embedded_fg = torch.tensor(embedded_fg).to(device)\n",
    "    embedded_fg = model_fg_zh.generate(embedded_fg, max_length=max_length,\n",
    "                                       num_beams=4, early_stopping=True)\n",
    "    for i in range(embedded_fg.size()[0]):\n",
    "        zh_texts.append(re.sub('<pad>| ', '', tokenizer_fg_zh.decode(embedded_fg[i])))\n",
    "\n",
    "    assert len(texts) == len(zh_texts)\n",
    "\n",
    "#     print(time.time() - t)\n",
    "\n",
    "    return zh_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-messaging",
   "metadata": {},
   "source": [
    "### 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incorrect-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'tcdata')\n",
    "files = {\n",
    "    'bq_corpus': os.path.join(data_dir, 'bq_corpus', 'train.tsv'),\n",
    "    'lcqmc': os.path.join(data_dir, 'lcqmc', 'train.tsv'),\n",
    "    'paws-x-zh': os.path.join(data_dir, 'paws-x-zh', 'train.tsv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "welsh-hampshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xiaobu-semantic-matching-2021-master/tcdata/bq_corpus/train.tsv\n",
      "/home/xiaobu-semantic-matching-2021-master/tcdata/lcqmc/train.tsv\n",
      "/home/xiaobu-semantic-matching-2021-master/tcdata/paws-x-zh/train.tsv\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for k, n in files.items():\n",
    "    try:\n",
    "        fr = open(n, 'r', encoding='utf8')\n",
    "    except:\n",
    "        fr = open(n, 'r', encoding='gbk')\n",
    "    tmp = []\n",
    "    for line in fr.readlines():\n",
    "        tmp.append(line.strip().split('\\t'))\n",
    "    data[k] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-photograph",
   "metadata": {},
   "source": [
    "### 翻译数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-direction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1001 [03:23<3:26:29, 12.58s/it]"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "for k, d in data.items():\n",
    "    fw_ze = open(os.path.join(data_dir, k, 'zh_en_aug_train.tsv'), 'a', encoding='utf8')\n",
    "    fw_zd = open(os.path.join(data_dir, k, 'zh_de_aug_train.tsv'), 'a', encoding='utf8')\n",
    "    n = int(len(d) / batch_size) + 1\n",
    "    for i in tqdm(range(n)):\n",
    "        texts = d[i * batch_size: (i + 1) * batch_size]\n",
    "        texts_a, texts_b, labels = zip(*texts)\n",
    "        if len(texts) == 0:\n",
    "            break\n",
    "        \n",
    "        zh_en_texts_a = batch_translation(texts_a, tokenizer_zh_en, model_zh_en, tokenizer_en_zh, model_en_zh)\n",
    "        zh_de_texts_a = batch_translation(texts_a, tokenizer_zh_de, model_zh_de, tokenizer_de_zh, model_de_zh)\n",
    "        zh_en_texts_b = batch_translation(texts_b, tokenizer_zh_en, model_zh_en, tokenizer_en_zh, model_en_zh)\n",
    "        zh_de_texts_b = batch_translation(texts_b, tokenizer_zh_de, model_zh_de, tokenizer_de_zh, model_de_zh)\n",
    "        zh_en_texts = list(zip(zh_en_texts_a, zh_en_texts_b, labels))\n",
    "        zh_de_texts = list(zip(zh_de_texts_a, zh_de_texts_b, labels))\n",
    "        zh_en_texts = list(map(lambda x: '\\t'.join(x) + '\\n', zh_en_texts))\n",
    "        zh_de_texts = list(map(lambda x: '\\t'.join(x) + '\\n', zh_de_texts))\n",
    "        \n",
    "        fw_ze.writelines(zh_en_texts)\n",
    "        fw_zd.writelines(zh_de_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-touch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
